\begin{frame}
	\frametitle{Workshops: Overview}

	\begin{itemize}
		\item Each workshop was planned to be a combination of lecture style teaching and practical exercises.
		\item All practical exercises were computer based and used R and Jags\footnote{Why Jags and not its alternatives? See below for discussion.}.
		\item Most lecture teaching involved R and Jags based demonstrations, which could be followed along step by step by attendees.
		\item Attendees were required to use their laptops, and details of how to install the required software were provided in advance.
		\item Source code and (most) other teaching materials are available at: \texttt{https://github.com/lawsofthought/priorexposure}.
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Workshop 1: Bayes for beginners}

	\begin{itemize}
		\item This workshop aimed to be a general introduction to Bayesian data analysis and how it differs from the more familiar classical approaches to data analysis.
		\item Here, we provided a gentle introduction to Bayesian methods. Topics included:
			\begin{itemize}
				\item Examples of Bayesian inference and using prior information in simple statistics problems.
				\item Understanding the likelihood function.
				\item Hypothesis testing using Bayes factors.
			\end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Sampling theory based inference}
	\framesubtitle{A binomial distribution with $n=250$ and $\theta=0.5$.}
	\input{binomial.sampling.distribution.tex}
	{\small
	\begin{itemize}
	\item Areas shaded in blue are values \emph{as or more extreme} than $m=139$.
	\end{itemize}
	}
\end{frame}

\begin{frame}
	\frametitle{The likelihood function versus sampling distribution}
		\begin{itemize}
			\item The sampling distribution is the binomial distribution:
				\[\Prob{m \given \theta,n} = \binom{n}{m} \theta^m (1-\theta)^{n-m}.\]
				This is always a function of $m$, with $\theta$ and $n$ fixed.
			\item The corresponding likelihood function treats the same function
				\[\binom{n}{m} \theta^m (1-\theta)^{n-m},\] 
				as a function of $\theta$, but now with $m$ and $n$ fixed.
			\item As such, the binomial likelihood is 
				\[L(\theta\given n, m) \propto \theta^m (1-\theta)^{n-m}.\]
\end{itemize}
\end{frame}



\begin{frame}
	\frametitle{Workshop 2: Doing Bayesian data analysis}

	\begin{itemize}
		\item This workshop aimed to provide a solid theoretical and practical foundation for real-world Bayesian data analysis in psychology and social sciences.
		\item Topics included:
			\begin{itemize}
				\item Some detailed examples of analytically tractable Bayesian inference (e.g. inference of Bernoulli random variables, inference of Poisson random variables, inference of means of univariate Normal models, etc.)
				\item Introduction to probabilistic modelling with Jags.
				\item Linear models with Jags.
			\end{itemize}

	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Workshop 3: Introduction to advanced Bayesian data analysis and Bayesian multilevel modelling}

	\begin{itemize}
		\item This workshop focused on advanced probabilistic modelling
			in Bayesian data analysis, and in particular, Bayesian
			data analysis using multilevel regression models.
		\item Topics included:
			\begin{itemize}
				\item Multilevel linear models.
				\item Multilevel generalized linear models, e.g. logistic regression, Poisson regression.
				\item Examples included models with categorical predictors, interactions, random slope and random intercept models, crossed and nested structures.
			\end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Workshop 4: Nonlinear and latent variable models}

	\begin{itemize}
		\item This final workshop focused on Bayesian latent variable modelling, particularly using mixture models, and nonlinear regression.
		\item Topics included:
			\begin{itemize}
				\item Nonlinear regression modelling using radial basis functions. 
				\item Nonlinear regression modelling using Gaussian processes.
				\item Finite mixture modelling.
				\item Nonparametric mixture modelling using Dirichlet processes. 
			\end{itemize}


	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{The extra workshops}

	\begin{itemize}
		\item Workshops 1 \& 2 proved very popular, and to meet demand, in 2016, we provided both workshops in April and again in June. We will repeat this this year.
		\item In addition, initially, we assumed a basic proficiency in R on the part of the workshop attendees. This was not generally true, and those who were less familiar with R struggled to keep up with exercises. As such, we put on one extra R workshop before each regular workshop pair.
		\item As such, we had $3 \times 3$ workshops in 2016 and again this year.
	\end{itemize}

\end{frame}
